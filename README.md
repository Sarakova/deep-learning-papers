# Deeplearning Papers (2016.11.9 ~) 
Deep learning paper list (ongoing)

<br>

## Google Brain Team

* Alireza Makhzani et al. <b> Adversarial Autoencoders </b> (2015.11) [[arXiv]] (https://arxiv.org/abs/1511.05644)

* Konstantinos Bousmalis et al. <b> Domain Separation Networks </b> (NIPS 2016) [[arXiv]] (https://arxiv.org/abs/1608.06019)

* Amit Daniely et al. <b> Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity </b> (2016.2) [[arXiv]] (https://arxiv.org/abs/1602.05897)

* Jimmy Ba et al. <b> Using Fast Weights to Attend to the Recent Past </b> (2016.10) [[arXiv]](https://arxiv.org/abs/1610.06258)

* S. M. Ali Eslami et al. <b> Attend, Infer, Repeat: Fast Scene Understanding with Generative Models </b> (2016.3) [[arXiv]] (https://arxiv.org/abs/1603.08575)






## DeepMind

* M Fraccaro et al. <b> Sequential Neural Models with Stochastic Layers </b> (NIPS 2016) [[arXiv]] (https://arxiv.org/abs/1605.07571)

* M Andrychowicz et al. <b> Learning to Learn by Gradient Descent by Gradient Descent </b> (NIPS 2016) [[arXiv]] (https://arxiv.org/abs/1606.04474)

* Max Jaderberg et al. <b> Decoupled Neural Interfaces using Synthetic Gradients </b> (2016.8) [[arXiv]] (https://arxiv.org/abs/1608.05343)

* A van den Oord et al <b> WaveNet: A Generative Model for Raw Audio </b> (2016.9) [[arXiv]] (https://arxiv.org/abs/1609.03499)



<br>


## Model
* Sergey Ioffe, Christian Szegedy. <b> Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift </b> (2015.11) [[arXiv]] (https://arxiv.org/abs/1502.03167)

* Tim Cooijmans et al. <b> Recurrent Batch Normalization </b> (2016.3) [[arXiv]] (https://arxiv.org/abs/1603.09025)

* Jimmy Ba et al. <b> Layer Normalization </b> (2016.7) [[arXiv]] (https://arxiv.org/abs/1607.06450)

* David Krueger et al. <b> Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations </b> (2016.11) [[ICLR 2017 open review]] (http://104.155.136.4:3000/forum?id=rJqBEPcxe)

* Gabriel Pereyra et al. <b> Regularizing Neural Networks by Penalizing Confident Output Distributions </b> (2016.11) [[ICLR 2017 open review]] (http://104.155.136.4:3000/forum?id=HkCjNI5ex)

* Yingce Xia et al. <b> Dual Learning for Machine Translation </b> (2016.11) [[arXiv]] (https://arxiv.org/abs/1611.00179)


<br>

## Generative Model

* Diederik P Kingma, Max Welling. <b> Auto-Encoding Variational Bayes </b> (2013.12) [[arXiv]] (https://arxiv.org/abs/1312.6114)

* Ian J. Goodfellow et al. <b> Generative Adversarial Networks </b> (2014.6) [[arXiv]] (https://arxiv.org/abs/1406.2661)

* Alec Radford, Luke Metz, Soumith Chintala. <b> Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks </b> (2015.11) [[arXiv]] (https://arxiv.org/abs/1511.06434)

* Yaniv Taigman, Adam Polyak, Lior Wolf. <b>Unsupervised Cross-Domain Image Generation</b> (2016.11) [[ICLR 2017 open review]] (http://104.155.136.4:3000/forum?id=Sk2Im59ex) 

<br>

## Sequence-to-Sequence Model
* Kyunghyun Cho et al. <b>Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation.</b> (2014.6) [[arXiv]](https://arxiv.org/abs/1406.1078) [[notes]] (https://github.com/yunjey/deeplearning-papers/blob/master/notes/learning_phrase_representation_using_rnn_enc_dec.md)

* Ilya Sutskever, Oriol Vinyals, Quoc V. Le. <b> Sequence to Sequence Learning with Neural Networks.</b> (2014.9) [[arXiv]](https://arxiv.org/abs/1409.3215) [[notes]] (https://github.com/yunjey/deeplearning-papers/blob/master/notes/seq2seq_with_nn.md)

* Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio. <b> Neural Machine Translation by Jointly Learning to Align and Translate.</b> (2014.9) [[arXiv]](https://arxiv.org/abs/1409.0473) [[notes]] (https://github.com/yunjey/deeplearning-papers/blob/master/notes/nmt_by_jointly_train_align_and_translate.md)

<br>

## Image Captioning
* Oriol Vinyals et al. <b> Show and Tell: A Neural Image Caption Generator </b> (2014.11) [[arXiv]] (https://arxiv.org/abs/1411.4555) 

* Xu Kelvin et al. <b> Show, attend and tell: Neural image caption generation with visual attention". </b> (2015.2) [[arXiv]] (https://arxiv.org/abs/1502.03044) [[notes]] (https://github.com/yunjey/deeplearning-papers/blob/master/notes/show_attend_and_tell.md)  [[tensorflow]] (https://github.com/yunjey/show-attend-and-tell-tensorflow)

* Oriol Vinyals et al. <b> Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge </b> (2016.9) [[arXiv]] (https://arxiv.org/abs/1609.06647) [[tensorflow]] (https://github.com/tensorflow/models/tree/master/im2txt)

* Jiasen Lu et al. <b> Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning </b> (2016.12) [[arXiv]] (https://arxiv.org/abs/1612.01887)

<br>

## Natural Language Processing

* Jason P.C. Chiu, Eric Nichols. <b> Named Entity Recognition with Bidirectional LSTM-CNNs </b> (2015.11) (ACL 2016) [[arXiv]] (https://arxiv.org/abs/1511.08308) 

* Jianpeng Cheng, Mirella Lapata. <b> Neural Summarization by Extracting Sentences and Words </b> (2016.3) (ACL 2016) [[arXiv]] (https://arxiv.org/abs/1603.07252)

* Iulian Vlad Serban et al. <b> Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus </b> (2016.3) (ACL 2016) [[arXiv]] (https://arxiv.org/abs/1603.06807) [[notes]] (https://github.com/yunjey/deeplearning-papers/blob/master/notes/generating_factoid_questions_with_rnn.md) 

* Karl Pichotta, Raymond J. Mooney. <b> Using Sentence-Level LSTM Language Models for Script Inference </b> (2016.8) (ACL 2016) [[arXiv]] (https://arxiv.org/abs/1604.02993)

* Yunchuan Chen et al. <b> Compressing Neural Language Models by Sparse Word Representations </b> (2016.10) (ACL 2016) [[arXiv]] (https://arxiv.org/abs/1610.03950)

