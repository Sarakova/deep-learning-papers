# Sequence to Sequence Learning with Neural Networks

The model proposed in this paper is very similar with <b> Kyunghyun Cho et al. Learning Phrase Representations using RNN Encoder-Decoder
for Statistical Machine Translation </b>


### Contributions

They present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure.

They found that multilayered deep LSTMs significantly outperformed shallow LSTMs.

They found it extremly valuable to reverse the order of the words of the input sequence but not output sequence. It allows the model to increase
BLEU socres from 25.9 to 30.6
